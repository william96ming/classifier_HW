%% Parameter
sigma = eye(3);
mu0 = [1;1;2];
mu1 = [1;1;-1.5];
mu2 = [1;1;4];
mu3 = [1;-1.5;2];
%% data distribution(class = 4, 3-dimensional)
%%generate data
p = 0.25; %uniform priors
N=10000;
[x_1000,labels_1000] = generateData(N);

%% MAP classifier
%likelihood
for i=1:N
Likelihood(i,1)=mvnpdf(x_1000(:,i),mu0,sigma);
Likelihood(i,2)= mvnpdf(x_1000(:,i),mu1,sigma);
Likelihood(i,3)= mvnpdf(x_1000(:,i),mu2,sigma);
Likelihood(i,4)= mvnpdf(x_1000(:,i),mu3,sigma);
end
%decision
for i=1:N
    if max(Likelihood(i,:)) == Likelihood(i,1)
        decision(i) = 0;
    elseif max(Likelihood(i,:)) == Likelihood(i,2)
        decision(i) = 1;
    elseif max(Likelihood(i,:)) == Likelihood(i,3)
        decision(i) = 2;
    else 
        decision(i) = 3;
    end
end

%error rate
error = sum(decision ~= labels_1000);
P_error = error/N;

%% gernerate data
N=100;
[x_100,labels_100] = generateData(N);
N=200;
[x_200,labels_200] = generateData(N);
N=500;
[x_500,labels_500] = generateData(N);
N=1000;
[x_1k,labels_1k] = generateData(N);
N=2000;
[x_2k,labels_2k] = generateData(N);
N=5000;
[x_5k,labels_5k] = generateData(N);
N=100000;
[x_100k,labels_100k] = generateData(N);
%%
test = int8(labels_1k);
test = test';
%% test error
[decision_100]= myNeuralNetworkFunction_100(x_100k);
decision_int_100 = int8(decision_100);

[decision_200]= myNeuralNetworkFunction_200(x_100k);
decision_int_200 = int8(decision_200);

[decision_500]= myNeuralNetworkFunction_500(x_100k);
decision_int_500 = int8(decision_500);

[decision_1000]= myNeuralNetworkFunction_1000(x_100k);
decision_int_1000 = int8(decision_1000);

[decision_2000]= myNeuralNetworkFunction_2000(x_100k);
decision_int_2000 = int8(decision_2000);

[decision_5000]= myNeuralNetworkFunction_5000(x_100k);
decision_int_5000 = int8(decision_5000);
%% error
    N=100000;
    error_100 = sum(decision_int_100 ~= labels_100k);
    P_error_100 = error_100/N;
    
    error_200 = sum(decision_int_200 ~= labels_100k);
    P_error_200 = error_200/N;
    
    error_500 = sum(decision_int_500 ~= labels_100k);
    P_error_500 = error_500/N;
    
    error_1000 = sum(decision_int_1000 ~= labels_100k);
    P_error_1000 = error_1000/N;
    
    error_2000 = sum(decision_int_2000 ~= labels_100k);
    P_error_2000 = error_2000/N;
    
    error_5000 = sum(decision_int_5000 ~= labels_100k);
    P_error_5000 = error_5000/N;
    
%% plot P-error vs # of training set
x = [100 200 500 1000 2000 5000];
y = [P_error_100 P_error_200 P_error_500 P_error_1000 0.3534 P_error_5000];
plot(x,y);
hold on;
xlabel('number of elements in training set');
ylabel('probability of error');

%%
hold on
axis([0 7 0 1])
xlabel('training size');
ylabel('probability of error');
title('P-error vs training size');
hold on;

plot(6,P_error,'r*')
%% MAP classifier
%likelihood
N=5000;
for i=1:N
Likelihood(i,1)=mvnpdf(x_5k(:,i),mu0,sigma);
Likelihood(i,2)= mvnpdf(x_5k(:,i),mu1,sigma);
Likelihood(i,3)= mvnpdf(x_5k(:,i),mu2,sigma);
Likelihood(i,4)= mvnpdf(x_5k(:,i),mu3,sigma);
end
%decision
for i=1:N
    if max(Likelihood(i,:)) == Likelihood(i,1)
        decision(i) = 0;
    elseif max(Likelihood(i,:)) == Likelihood(i,2)
        decision(i) = 1;
    elseif max(Likelihood(i,:)) == Likelihood(i,3)
        decision(i) = 2;
    else 
        decision(i) = 3;
    end
end

%error rate
error = sum(decision ~= labels_5k);
P_error = error/N;

%%
function [x,labels] = generateData(N)
%N = 100;
figure(1), clf,     %colors = 'bm'; markers = 'o+';
classPriors = [0.25,0.25,0.25,0.25];

for i = 1:N
if rand < 0.5
    if rand < 0.5
        labels(i)=0;
    else
        labels(i)=1;
    end
else
    if rand < 0.5
        labels(i)=2;
    else
        labels(i)=3;
    end
end
end

indl_0 = find(labels==0);
indl_1 = find(labels==1);
indl_2 = find(labels==2);
indl_3 = find(labels==3);

for l = 0:3
    sigma = eye(3);
    mu0 = [1;1;2];
    mu1 = [1;1;-1.5];
    mu2 = [1;1;4];
    mu3 = [1;-1.5;2];
    if l == 0
        N0 = length(indl_0);
        x(:,indl_0) = mvnrnd(mu0,sigma,N0)';
        scatter3(x(1,indl_0),x(2,indl_0),x(3,indl_0),'b+'), hold on,
        axis equal,
        
    elseif l == 1
        N1 = length(indl_1);
        x(:,indl_1) = mvnrnd(mu1,sigma,N1)';
        scatter3(x(1,indl_1),x(2,indl_1),x(3,indl_1),'r+'), hold on,
        axis equal,
        
    elseif l == 2
        N2 = length(indl_2);
        x(:,indl_2) = mvnrnd(mu2,sigma,N2)';
        scatter3(x(1,indl_2),x(2,indl_2),x(3,indl_2),'y+'), hold on,
        axis equal,
        
    elseif l == 3
        N3 = length(indl_3);
        x(:,indl_3) = mvnrnd(mu3,sigma,N3)';
        scatter3(x(1,indl_3),x(2,indl_3),x(3,indl_3),'g+'), hold on,
        axis equal,
    end
end
end
%%%
function [x,labels] = generateDataFromGMM(N,gmmParameters)
% Generates N vector samples from the specified mixture of Gaussians
% Returns samples and their component labels
% Data dimensionality is determined by the size of mu/Sigma parameters
priors = gmmParameters.priors; % priors should be a row vector
meanVectors = gmmParameters.meanVectors;
covMatrices = gmmParameters.covMatrices;
n = size(gmmParameters.meanVectors,1); % Data dimensionality
C = length(priors); % Number of components
x = zeros(n,N); labels = zeros(1,N); 
% Decide randomly which samples will come from each component
u = rand(1,N); thresholds = [cumsum(priors),1];
for l = 1:C
    indl = find(u <= thresholds(l)); Nl = length(indl);
    labels(1,indl) = l*ones(1,Nl);
    u(1,indl) = 1.1*ones(1,Nl); % these samples should not be used again
    x(:,indl) = mvnrnd(meanVectors(:,l),covMatrices(:,:,l),Nl)';
end
end

function [y1] = myNeuralNetworkFunction_100(x1)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Auto-generated by MATLAB, 12-Nov-2021 19:44:33.
%
% [y1] = myNeuralNetworkFunction(x1) takes these arguments:
%   x = 3xQ matrix, input #1
% and returns:
%   y = 1xQ matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Input 1
x1_step1.xoffset = [-1.84652307109205;-3.03694596787881;-3.17545817618487];
x1_step1.gain = [0.296756490717165;0.308982319556789;0.223142093042129];
x1_step1.ymin = -1;

% Layer 1
b1 = [-3.4603124524222637071;3.1115322570144252623;2.4337178263624243968;1.8956637850852438021;-1.534587028654059937;-0.6280314063342699038;0.50452920835328984861;0.15043756538266397915;-0.47576409047437706601;-0.94844607404628522573;-1.0880456424455164921;-1.579985359017430202;2.5710311018275460171;3.3033934790442383012;3.0666879305126606425];
IW1_1 = [2.5474800726702668818 -0.21146244056789592625 2.3097648843916087635;-2.3228890353942386326 0.20356828282466629676 -2.3747379579021248297;-2.1950762045111757992 2.6497396818357921155 -0.43617748212420881515;-2.2640727130210027696 -0.1742133665709476309 -2.6400454925309615462;2.9700767196229826972 0.75601639528229569454 -1.3260430437155181771;2.3695273566766021922 2.521565766115252849 -0.61637979079047888753;-1.2512189009713501342 1.4919741131588435401 -2.8856293162145481013;-1.0534083597294949808 0.37213600167579147238 3.2858580081826516128;-2.6222031964974141793 -0.98420487878638529544 1.9681208086146635416;-1.7169111126590101968 -2.2278767685849327762 1.6527242147555405438;-2.669483788571798577 1.7667414624103541243 1.2852947020996294825;-0.81613673482526472025 -2.5475995450249011576 -2.1417990911149908939;0.50333497889517486712 2.2176018883597574671 2.5318503473911744273;1.3231217506054027133 2.7876347576746134393 0.74018334077762015788;2.7591454762615152951 -1.6556897861063939459 -1.8402196211512908963];

% Layer 2
b2 = 0.45715076374905894152;
LW2_1 = [-0.045642614073129153529 -0.19692048323655025888 -0.27413744744603552039 0.40578959027808514382 -0.51061783580847697195 0.50155905552386181867 -0.9861838890521164025 -0.49724592446823057124 0.31497164094790874866 0.15116763051904952286 -0.051630034127383955811 0.63001586216670923957 0.057410690603143771371 0.1252937043204509493 -0.14774379486040417619];

% Output 1
y1_step1.ymin = -1;
y1_step1.gain = 0.666666666666667;
y1_step1.xoffset = 0;

% ===== SIMULATION ========

% Dimensions
Q = size(x1,2); % samples

% Input 1
xp1 = mapminmax_apply(x1,x1_step1);

% Layer 1
a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);

% Layer 2
a2 = repmat(b2,1,Q) + LW2_1*a1;

% Output 1
y1 = mapminmax_reverse(a2,y1_step1);
end

function [y1] = myNeuralNetworkFunction_200(x1)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Auto-generated by MATLAB, 12-Nov-2021 19:48:03.
%
% [y1] = myNeuralNetworkFunction(x1) takes these arguments:
%   x = 3xQ matrix, input #1
% and returns:
%   y = 1xQ matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Input 1
x1_step1.xoffset = [-2.32051874267831;-3.41247043670182;-5.13264769683809];
x1_step1.gain = [0.302753312405005;0.299024096865229;0.163821093971796];
x1_step1.ymin = -1;

% Layer 1
b1 = [-3.4200388138941111293;3.0986605763735530772;-2.1116844242658054931;2.1345222507340344542;-0.89378590530284085514;-0.37163811212346692381;-0.77491365845690607372;-0.046827540471865762572;-0.6261830912767959445;-0.82065887521957636697;1.9093933540422922679;-1.5613413424141018293;2.3463061056724665576;2.9517905703200688627;3.3923731809398245396];
IW1_1 = [0.86940051314590816034 -1.3495884625904954746 -3.0790792703704008559;-3.1563946356325636167 0.0014355743283686543243 1.0547086869882436044;0.49562192828952800205 2.9802327539526625166 3.3737638096577735602;-1.3482116434299800112 2.2257240538804898833 2.1666147389979584226;-0.74169776589235481001 -4.7182027280882969933 0.92699781419160232065;2.3218354056430285937 -0.35278632822998834406 -2.9335149649192020505;-1.8575405899430894596 -1.6744154782275122439 4.194896790202954584;0.66268461321279514831 -0.61093137916482664895 -4.0809126357538625385;-0.90558052380569975526 -3.8989202467702588351 2.0494680081730396459;-1.3202181754539281666 -3.5783248510614416027 -1.8468563923177045982;1.8694039921498462054 -1.7995982441180471412 -1.8846282345452436413;-0.60561205755957503438 -2.4982880987345392754 -2.704309672165772227;0.15352037624698364193 1.895408918127825082 2.8464407975305667797;0.52271597425208438104 -3.2218879092343075676 0.6977602943393386159;1.3459088194103472347 -1.9438501455523786721 2.5072671792220670994];

% Layer 2
b2 = -0.28818842154172347181;
LW2_1 = [-0.23488430361853587791 0.40401014154316799987 0.75561512706781486237 -0.2879764615077116674 0.8687264621032494416 -0.10340830508974818702 0.58427823125495814693 0.97056360331771673877 -0.051552235180856914587 0.065516068183961620464 0.1150813577309636182 -0.68627288033221311991 0.39701303479309424382 0.36143436703518333486 -0.39112421214017345505];

% Output 1
y1_step1.ymin = -1;
y1_step1.gain = 0.666666666666667;
y1_step1.xoffset = 0;

% ===== SIMULATION ========

% Dimensions
Q = size(x1,2); % samples

% Input 1
xp1 = mapminmax_apply(x1,x1_step1);

% Layer 1
a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);

% Layer 2
a2 = repmat(b2,1,Q) + LW2_1*a1;

% Output 1
y1 = mapminmax_reverse(a2,y1_step1);
end

function [y1] = myNeuralNetworkFunction_500(x1)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Auto-generated by MATLAB, 12-Nov-2021 19:53:26.
%
% [y1] = myNeuralNetworkFunction(x1) takes these arguments:
%   x = 3xQ matrix, input #1
% and returns:
%   y = 1xQ matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Input 1
x1_step1.xoffset = [-2.04192640122702;-4.16436202530525;-4.98130165060815];
x1_step1.gain = [0.339356166094546;0.258550558454263;0.175117930931281];
x1_step1.ymin = -1;

% Layer 1
b1 = [-3.9755345818463143637;3.416573045846664769;2.5407943280601750224;-2.5031805095542263295;1.4380584422375974896;-0.71038448602289672174;0.53264586940554747851;0.051611738561986586493;-0.36047561413155448928;1.0698521380971728334;-0.22350673376521351665;2.6346497277256557545;2.5669366989827628878;3.0300339760620564888;3.4788132495785428766];
IW1_1 = [2.5913971045189043707 0.51025296332928282705 -1.4243791204131679429;-2.2787306113887968451 1.9208059922472786596 1.0174050168596344701;-1.0572651153113992173 -1.0725319773594688133 -3.0671206843740255188;2.2147601519572615025 -0.87358505107477446572 1.9819763701059518457;-1.13685320396984757 0.12314520367485108643 -3.1491096516719121468;2.7094451957498910666 -0.25171268774390420653 2.1060380361045498709;-2.6358586149866880532 0.24095832882566994848 -2.0421863169154930162;2.3539378739323106871 1.8661798676037046896 -1.8601122338626057129;-0.73385862807114210504 -2.6246167527390129592 2.3767127060227313251;0.15810810973273164981 0.90120181796876719371 3.0826212953716405352;-0.69821599854849325517 2.3566160300113674353 3.0556659202830283029;2.6458404496321907828 -0.45925835138831927473 1.3578759567658531537;2.3211721199660675374 -2.1766863287936506488 1.1629088395351094842;2.4051368221396578484 2.337149568104060382 0.30617918540105093506;1.1438291364390111848 -3.1794537908548585747 -0.47857233360131706812];

% Layer 2
b2 = 0.9333031972241452312;
LW2_1 = [0.38364795343999030797 0.089504473083134361566 -0.19736859541060042256 -0.30947269237707664136 -0.10879473859396110447 0.0041785980372491459897 0.061785923436821960086 0.34641965231616117515 0.99894121727286766532 0.13418509190495619965 -0.38583283701976306412 0.18084902589922030591 -0.43417289318523905051 -0.17744731523550236196 -0.20625147402204449532];

% Output 1
y1_step1.ymin = -1;
y1_step1.gain = 0.666666666666667;
y1_step1.xoffset = 0;

% ===== SIMULATION ========

% Dimensions
Q = size(x1,2); % samples

% Input 1
xp1 = mapminmax_apply(x1,x1_step1);

% Layer 1
a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);

% Layer 2
a2 = repmat(b2,1,Q) + LW2_1*a1;

% Output 1
y1 = mapminmax_reverse(a2,y1_step1);
end

function [y1] = myNeuralNetworkFunction_1000(x1)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Auto-generated by MATLAB, 12-Nov-2021 19:57:46.
%
% [y1] = myNeuralNetworkFunction(x1) takes these arguments:
%   x = 3xQ matrix, input #1
% and returns:
%   y = 1xQ matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Input 1
x1_step1.xoffset = [-3.0317414335182;-4.81585214725332;-4.62023140099887];
x1_step1.gain = [0.2708560223805;0.212856004223965;0.170196250116901];
x1_step1.ymin = -1;

% Layer 1
b1 = [-2.8769340519309696269;2.9431527060992062061;-1.4677887164110678953;1.9484384806118633104;-1.2956144079686555681;-1.1817741960213250518;0.11524775060616924971;-0.1536307566663129287;0.046572117172145745256;0.51436856639024330917;-1.2560030435056743769;2.1411281525748444565;-2.6625506878923972387;-3.3477282723472483461;3.3664133778540792363];
IW1_1 = [0.48662938939508226799 2.8003611709922737738 2.3416001552082432724;-1.0177500200128155594 1.4757743667526164177 2.9369692827188260686;0.49488468813607777719 3.792017375346652841 -0.38186763704638276851;-1.9162601999581814205 -2.3471722098703176052 -1.5921445641043872055;0.37865235320991919021 -1.2058319430494319757 -3.2261637573532846446;-0.033599768177156673354 0.32597281308640280173 3.8185530850108913015;0.19646591831706189391 -2.5601325519110633699 -2.7960572447823999198;-1.7611127871994622929 2.264926641030760468 1.855694045764454847;0.18399330059325880171 2.9765188569462570634 -2.688885769490767963;3.2690063645366262968 -0.47915389974024097386 -0.15954997761054742766;-0.52496309164718701901 2.269604707086177342 -2.348397314866386143;1.7532979038698492946 -0.53456283180674912803 -2.8027786276462069992;-2.8913762514513474677 -0.85712445175834106603 -1.4477231202516755992;-0.65937937214832786914 -3.1407888311113132396 -0.45167809391646762673;3.482102381788110268 -0.36582580895087568607 -0.27309358811360606722];

% Layer 2
b2 = -0.044193838990656000854;
LW2_1 = [0.00062510693600056899122 -0.20682503801974466695 0.45965611329059846701 -0.078412626943112545552 -0.23920754313328476681 0.43094744442362659109 0.94436990125862352308 0.086515618492215479174 -0.72138618842768842399 -0.11735676769405700703 -0.062115292323462814594 0.23831240855284716029 -0.30817432998865923466 -0.92053692910906048663 -0.72479515960865403823];

% Output 1
y1_step1.ymin = -1;
y1_step1.gain = 0.666666666666667;
y1_step1.xoffset = 0;

% ===== SIMULATION ========

% Dimensions
Q = size(x1,2); % samples

% Input 1
xp1 = mapminmax_apply(x1,x1_step1);

% Layer 1
a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);

% Layer 2
a2 = repmat(b2,1,Q) + LW2_1*a1;

% Output 1
y1 = mapminmax_reverse(a2,y1_step1);
end

function [y1] = myNeuralNetworkFunction_2000(x1)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Auto-generated by MATLAB, 12-Nov-2021 20:00:31.
%
% [y1] = myNeuralNetworkFunction(x1) takes these arguments:
%   x = 3xQ matrix, input #1
% and returns:
%   y = 1xQ matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Input 1
x1_step1.xoffset = [-2.14805837233321;-4.95260820226841;-4.58332357932299];
x1_step1.gain = [0.299381878279866;0.222110730254003;0.174566804168783];
x1_step1.ymin = -1;

% Layer 1
b1 = [3.6330885100612229088;-3.214161840385022284;-2.5721285805948568282;-2.4185855721577596;-1.051020762298145117;-1.5431765192488997585;-0.97763887431389484561;-0.0012651429441195502501;-0.024938519061085930723;0.76072101724201612871;-2.380600376775942717;1.7020288124749076708;-1.8687520742807157692;2.6868297374334502159;3.3976776755219622039];
IW1_1 = [-2.0319587112859540667 1.0831558513558032786 2.4016270835070381295;2.4289079636457331368 1.8595070677821345928 -0.89906482842511081799;0.24928722289576021875 -3.2118619414077653751 0.81787454060153674806;0.91403056079308830917 -2.3235593223239643379 1.9787426154872203021;0.62245285596953470186 3.0954276531882611856 1.7842503439515036145;0.39705279863629538983 1.2379213267604929349 -3.1946336005685713744;3.0285041220849162613 -1.2190933609566170226 -0.050913252257513058607;-0.34477117662499906503 2.2719689255052810672 -3.1073626549914616923;0.10350996492668931848 3.12666428440171007 -1.9343764387435646857;0.65540068580934063291 -2.2572871633052118412 -1.7981850001735002476;-2.4140863662041032178 -0.45984895339730402775 -1.7801126950639964974;3.1507982085257375537 0.8919488510353852373 0.97161548154122123755;0.55260581719512102961 4.1795751683093005369 1.7433162768510734875;1.007085004511732329 -0.63726401507496999166 3.1951069668144844726;1.496336256327087666 -0.1451293277804667492 3.2249937875206318161];

% Layer 2
b2 = -0.22698438567340203931;
LW2_1 = [0.35811495530357195793 0.049636600833518951248 -0.00064520572038556829597 -0.15790106118649352429 -1.3247467331724072093 0.39346224545995844846 -0.021162962721879709227 -0.60210258985619913297 -0.3581371256264127001 -0.0038899548814882523376 1.0745186149968357636 0.38358259988598447654 1.2253239716968500339 0.2352078531644256898 0.83828290027330476519];

% Output 1
y1_step1.ymin = -1;
y1_step1.gain = 0.666666666666667;
y1_step1.xoffset = 0;

% ===== SIMULATION ========

% Dimensions
Q = size(x1,2); % samples

% Input 1
xp1 = mapminmax_apply(x1,x1_step1);

% Layer 1
a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);

% Layer 2
a2 = repmat(b2,1,Q) + LW2_1*a1;

% Output 1
y1 = mapminmax_reverse(a2,y1_step1);
end

function [y1] = myNeuralNetworkFunction_5000(x1)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Auto-generated by MATLAB, 12-Nov-2021 20:02:57.
%
% [y1] = myNeuralNetworkFunction(x1) takes these arguments:
%   x = 3xQ matrix, input #1
% and returns:
%   y = 1xQ matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Input 1
x1_step1.xoffset = [-3.0197163753905;-4.7162451372651;-4.65821815542448];
x1_step1.gain = [0.262832045381717;0.207287918154092;0.16761564603322];
x1_step1.ymin = -1;

% Layer 1
b1 = [1.7768127216011100344;-1.3112749454040655284;1.9332614041794087267;1.7459288728684265735;-0.38297429846230535144;-0.81860751528602793137;0.55360250614798311997;0.0066774391248489162187;0.61879029865120449561;0.23242416589120501458;1.7524833845027731893;-1.9795822391875617097;3.1171356543256036886;2.6125850188267025764;3.4282045923158941392];
IW1_1 = [-0.11882890319232355947 -1.8963448255596684344 -4.255812704393976631;1.2838958029974363306 3.7663075649045474336 -0.32333447287439848195;-2.9780039167603740147 1.013857738789005758 -1.5847354718861783596;0.24126512373041431037 3.7288979830818305139 1.3817640805165747331;-0.55029186285087727804 2.142924592363140146 2.972702304539715179;0.27049857069010924127 -3.3240440230274268529 -0.13134671067387693189;-1.804500459746012142 -0.37506990956039704121 -1.4526842062028331171;-2.1620943666941720984 2.0653614644490407848 1.4064299540764266805;2.8230740165041634526 -1.9398223332020216869 0.76224791226590404847;0.11938945046827964536 3.41111141973588472 -3.8348472971990554825;2.2249136914561766787 1.4976905533879145693 -1.8868007367944426012;-2.247671352332421435 1.9734907111566490912 1.641104933411023703;2.9466234048828625625 -0.35569076732874510549 0.54842035967584412681;0.66618573280288739813 0.33761925415630306135 -3.5705383452302039693;1.4711633789344087386 2.9894175020400872711 -0.94786225646250299448];

% Layer 2
b2 = 0.25421431372826952533;
LW2_1 = [-0.66470393291333107477 0.27845468496459779839 -0.081614266818984163021 0.54117760744312781718 -0.72986603674007466402 0.75997960583459445161 0.33773586719503823073 0.26169882669744859438 0.12735178708992275776 -0.58031768086768975579 0.12510371505631012745 0.14092910615609474667 -0.20706522423229775653 0.39105249768955635536 -0.05118777312356451864];

% Output 1
y1_step1.ymin = -1;
y1_step1.gain = 0.666666666666667;
y1_step1.xoffset = 0;

% ===== SIMULATION ========

% Dimensions
Q = size(x1,2); % samples

% Input 1
xp1 = mapminmax_apply(x1,x1_step1);

% Layer 1
a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);

% Layer 2
a2 = repmat(b2,1,Q) + LW2_1*a1;

% Output 1
y1 = mapminmax_reverse(a2,y1_step1);
end

% ===== MODULE FUNCTIONS ========

% Map Minimum and Maximum Input Processing Function
function y = mapminmax_apply(x,settings)
y = bsxfun(@minus,x,settings.xoffset);
y = bsxfun(@times,y,settings.gain);
y = bsxfun(@plus,y,settings.ymin);
end

% Sigmoid Symmetric Transfer Function
function a = tansig_apply(n,~)
a = 2 ./ (1 + exp(-2*n)) - 1;
end

% Map Minimum and Maximum Output Reverse-Processing Function
function x = mapminmax_reverse(y,settings)
x = bsxfun(@minus,y,settings.ymin);
x = bsxfun(@rdivide,x,settings.gain);
x = bsxfun(@plus,x,settings.xoffset);
end
